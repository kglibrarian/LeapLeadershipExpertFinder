{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import io\n",
    "from config import api_key_inCites\n",
    "from config import api_key_WOS\n",
    "from collections import OrderedDict\n",
    "from pandas import json_normalize\n",
    "import base64\n",
    "import xmltodict\n",
    "import time \n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##InCites Resources: \n",
    "#https://clarivate.com/webofsciencegroup/solutions/xml-and-apis/\n",
    "#https://developer.clarivate.com/help\n",
    "#https://api.clarivate.com/api/incites/DocumentLevelMetricsByUT/json\n",
    "#https://github.com/Clarivate-SAR/incites-retrieve\n",
    "#https://api.clarivate.com/swagger-ui/?url=https%3A%2F%2Fdeveloper.clarivate.com%2Fapis%2Fincites%2Fswagger%3FforUser%3D9c13dcee882598956b564212f82c2236a51e3f56\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload a .csv from Elements Reporting Database\n",
    "\n",
    "## \"data/2022_03-22 FSM Publications 2017-2022.csv\"\n",
    "## \"data/utpractice.csv\"\n",
    "\n",
    "fsm_elements_publications_path = \"data/2022_03-22 FSM Publications 2017-2022.csv\"\n",
    "\n",
    "## Read the CSV file and store into Pandas DataFrame with the column Scopus Author ID as a string\n",
    "fsm_elements_df = pd.read_csv(fsm_elements_publications_path, encoding = \"ISO-8859-1\", na_values=['NULL', '<NA>'])\n",
    "\n",
    "#Change the column names to lower case with underscore for spaces\n",
    "fsm_elements_df.columns =  fsm_elements_df.columns.str.strip().str.lower().str.replace(\" \", \"_\").str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "fsm_elements_df.head()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The function \"prep_UT_list\" takes in the UT_list_df dataframe and formats the \"id\" column \n",
    "#as a string, then uses the column to create a list, removes any of the \"nan\" values lines that don't \n",
    "#have an ID, and finally returns a list called \"cleaned_UT_list\"\n",
    "    \n",
    "## Change the data type in the dataframe column called \"web_of_science\" to a string. \n",
    "fsm_elements_df['web_of_science'] = fsm_elements_df['web_of_science'].astype(str)\n",
    "\n",
    "## Save the column called id to a list called interim_UT_list\n",
    "interim_UT_list = fsm_elements_df['web_of_science'].tolist()\n",
    "# print(interim_UT_list)\n",
    "\n",
    "# Remove nan values from list\n",
    "interim_UT_list_2 = [x for x in interim_UT_list if str(x) != 'nan']\n",
    "\n",
    "## Remove duplicates from interim_UT_list\n",
    "interim_UT_list_3 = list(set(interim_UT_list_2))\n",
    "# print(interim_UT_list_2)\n",
    "\n",
    "#Remove the WOS: characters from each item in the list\n",
    "cleaned_UT_list = [i.replace(\"WOS:\", \"\") for i in interim_UT_list_3]\n",
    "# print(cleaned_UT_list)\n",
    "\n",
    "## Resources\n",
    "## https://www.geeksforgeeks.org/python-ways-to-remove-duplicates-from-list/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chunk list of identifiers for batch api query\n",
    "\n",
    "## Create variables\n",
    "chunks = []\n",
    "n = 100\n",
    "\n",
    "## Create for loop with range\n",
    "## python's range function takes (start, stop, step)\n",
    "\n",
    "for i in range(0, len(cleaned_UT_list), n):\n",
    "    \n",
    "    ## print(i)\n",
    "    \n",
    "    ## Reference items in a list by using [start:stop] with numbers\n",
    "    ## example is:  0 : 0 + 100 = start list at 0 and end list at 0 + 100\n",
    "    chunk = cleaned_UT_list[i:i + n]\n",
    "    \n",
    "    ## Append each list chunk into a larger list called chunks (i.e. a list of lists)\n",
    "    chunks.append(chunk)\n",
    "       \n",
    "\n",
    "## Inspect result\n",
    "# print(len(chunks))\n",
    "# print(chunks)\n",
    "\n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn each chunk list into a string separated by commas\n",
    "\n",
    "## Create variables\n",
    "multiple_strings = []\n",
    "\n",
    "## Use for loop to access each chunk to turn it into a string\n",
    "## Essentially creating a list of strings (rather than a list of lists)\n",
    "\n",
    "for chunk in chunks: \n",
    "    \n",
    "    ## Use python's join method to join items from a list into one string separated by a comma\n",
    "    single_string = \",\".join(chunk)\n",
    "    \n",
    "    ## Inspect Result\n",
    "    #print(type(single_string))\n",
    "    #print(single_string)\n",
    "    \n",
    "    ## Append each new string to a list, to create a list of strings \n",
    "    multiple_strings.append(single_string)\n",
    "\n",
    "## Inspect Result\n",
    "print(type(multiple_strings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create API call to InCites using chunks\n",
    "\n",
    "\n",
    "## Create variables\n",
    "multiple_chunk_list = []\n",
    "single_chunk_dict = {}\n",
    "BATCH_SIZE= 100\n",
    "\n",
    "## Create for loop \n",
    "\n",
    "for item in multiple_strings: \n",
    "        \n",
    "    url = \"https://api.clarivate.com/api/incites/DocumentLevelMetricsByUT/json\"\n",
    "    headers = {\n",
    "            'X-ApiKey': api_key_inCites,\n",
    "            'Accept':'application/json'\n",
    "            }\n",
    "\n",
    "    parameters = {\n",
    "            'UT' : item,\n",
    "            'batch': n,\n",
    "            \"ver\": 2,\n",
    "            \"schema\": \"ct\", \n",
    "            \"esci\": \"y\",                  \n",
    "            }\n",
    "        \n",
    "        \n",
    "    try:\n",
    "         \n",
    "        ## Python time method sleep() suspends execution for the given number of seconds \n",
    "        time.sleep(0.1) \n",
    "\n",
    "        ## Make the API request \n",
    "        single_chunk_response = requests.get(url, headers=headers, params=parameters)\n",
    "        \n",
    "        ## Inspect result\n",
    "        #print(single_chunk_response.url)\n",
    "        #print(single_chunk_response.status_code)\n",
    "\n",
    "        ## Use the response library's .json() function to returns a JSON object of the api response\n",
    "        single_chunk_dict = single_chunk_response.json()\n",
    "        \n",
    "            \n",
    "        ## Append each single_chunk_dict to multiple_chunk_list to create a list of dictionaries\n",
    "        multiple_chunk_list.append(single_chunk_dict.copy())\n",
    "    \n",
    "    except (KeyError, IndexError):\n",
    "        print(\"Missing field/result... skipping.\")\n",
    "        print(\"------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect the result\n",
    "print(len(multiple_chunk_list))\n",
    "print(type(multiple_chunk_list))\n",
    "# print(multiple_chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use json_normalize to flatten the json contained in the \"api\" field. \n",
    " \n",
    "incites_df = pd.DataFrame.from_dict(json_normalize(multiple_chunk_list, meta=[\"api\"], record_path=[\"api\", \"rval\"]),orient=\"columns\")\n",
    "# incites_df.head()\n",
    "\n",
    "\n",
    "#References\n",
    "#https://stackoverflow.com/questions/48177934/flatten-or-unpack-list-of-nested-dicts-in-dataframe\n",
    "#https://stackoverflow.com/questions/50161070/convert-list-of-dicts-of-dict-into-dataframe\n",
    "#https://stackoverflow.com/questions/43984865/python-having-trouble-returning-a-pandas-data-frame-from-a-user-defined-functio\n",
    "#https://stackoverflow.com/questions/37668291/flatten-double-nested-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flattens the json contained in the PERCENTILE field and creates series then columns of the content. The funciton returns a pandas dataframe\n",
    "\n",
    "remove_percentile_nest = pd.concat([incites_df.drop(['PERCENTILE'], axis=1), incites_df['PERCENTILE'].apply(pd.Series)], axis=1, join=\"outer\")\n",
    "remove_percentile_nest.rename(columns= {0:'CTMacro', 1:'CTMeso', 2:'CTMicro' }, inplace=True)\n",
    "# remove_percentile_nest.head()\n",
    "\n",
    "#References\n",
    "#https://stackoverflow.com/questions/29325458/dictionary-column-in-pandas-dataframe/29330853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flattens the json contained in the CT Macro, Meso, Micro columns and creates series then columns of the content. \n",
    "remove_macro_nest = pd.concat([remove_percentile_nest.drop(['CTMacro'], axis=1), remove_percentile_nest['CTMacro'].apply(pd.Series)], axis=1, join=\"outer\")\n",
    "remove_macro_nest.rename(columns= {\"CODE\":'CTMacro_Code', \n",
    "                                   \"CAT_PERC\":'CTMacro_Cat_Perc', \n",
    "                                   \"SUBJECT\":'CTMacro_Subj',\n",
    "                                   \"CAT_EXP_CITATION\": \"CTMacro_CatExpCitation\",\n",
    "                                   \"LEVEL\": \"CTMacro_Level\",\n",
    "                                   \"IS_BEST\": \"CTMacro_IsBest\",\n",
    "                                   \"CNCI\": \"CTMacro_CNCI\"\n",
    "                                  }, inplace=True)\n",
    "# remove_macro_nest.head()\n",
    "\n",
    "\n",
    "remove_meso_nest = pd.concat([remove_macro_nest.drop(['CTMeso'], axis=1), remove_macro_nest['CTMeso'].apply(pd.Series)], axis=1, join=\"outer\")\n",
    "remove_meso_nest.rename(columns= {\"CODE\":'CTMeso_Code', \n",
    "                                   \"CAT_PERC\":'CTMeso_Cat_Perc', \n",
    "                                   \"SUBJECT\":'CTMeso_Subj',\n",
    "                                   \"CAT_EXP_CITATION\": \"CTMeso_CatExpCitation\",\n",
    "                                   \"LEVEL\": \"CTMeso_Level\",\n",
    "                                   \"IS_BEST\": \"CTMeso_IsBest\",\n",
    "                                   \"CNCI\": \"CTMeso_CNCI\"\n",
    "                                  }, inplace=True)\n",
    "# remove_meso_nest.head()\n",
    "\n",
    "remove_micro_nest = pd.concat([remove_meso_nest.drop(['CTMicro'], axis=1), remove_meso_nest['CTMicro'].apply(pd.Series)], axis=1, join=\"outer\")\n",
    "remove_micro_nest.rename(columns= {\"CODE\":'CTMicro_Code', \n",
    "                                   \"CAT_PERC\":'CTMicro_Cat_Perc', \n",
    "                                   \"SUBJECT\":'CTMicro_Subj',\n",
    "                                   \"CAT_EXP_CITATION\": \"CTMicro_CatExpCitation\",\n",
    "                                   \"LEVEL\": \"CTMicro_Level\",\n",
    "                                   \"IS_BEST\": \"CTMicro_IsBest\",\n",
    "                                   \"CNCI\": \"CTMicro_CNCI\"\n",
    "                                  }, inplace=True)\n",
    "# remove_micro_nest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace all occurring digits in the strings with nothing\"\n",
    "\n",
    "remove_micro_nest['CTMacro_Subj'] = remove_micro_nest['CTMacro_Subj'].str.replace('\\d+','').str.lstrip(\"..\")\n",
    "remove_micro_nest['CTMeso_Subj'] = remove_micro_nest['CTMeso_Subj'].str.replace('\\d+','').str.lstrip(\"..\")\n",
    "remove_micro_nest['CTMicro_Subj'] = remove_micro_nest['CTMicro_Subj'].str.replace('\\d+','').str.lstrip(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the fsm_elements_df by adding a column for assession number\n",
    "\n",
    "fsm_elements_df[\"ACCESSION_NUMBER\"] = fsm_elements_df[\"web_of_science\"].str.lstrip(\"WOS:\")\n",
    "fsm_elements_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(fsm_elements_df, remove_micro_nest, how=\"left\", left_on='ACCESSION_NUMBER', right_on='ACCESSION_NUMBER')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export file to excel, without the Pandas index, but with the header\n",
    "\n",
    "merged_df.to_excel(\"output/merged.xlsx\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows that don't have a citation topic\n",
    "\n",
    "merged_df.dropna(subset=[\"CTMicro_Subj\"], inplace=True)  \n",
    "new_merged_df = merged_df[merged_df[\"CTMicro_Subj\"].str.contains(\"No Topic assigned\") == False]\n",
    "\n",
    "\n",
    "new_merged_df.head()\n",
    "\n",
    "## Resources\n",
    "## https://www.statology.org/pandas-drop-rows-that-contain-string/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating the columns\n",
    "# for col in merged_df.columns:\n",
    "#     print(col)\n",
    "\n",
    "new_merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group by NetID and sum number of pubs from search results\n",
    "\n",
    "groupby_df = new_merged_df.groupby('netid').agg({'name': 'first', \n",
    "                                             'position' : 'first',\n",
    "                                             'department': 'first', \n",
    "                                             'school': 'first', \n",
    "                                             'employee_id': 'first',\n",
    "                                             'is_current_staff' : 'first',\n",
    "                                             'is_academic' : 'first',\n",
    "                                             'ACCESSION_NUMBER': lambda x: list(x),\n",
    "                                             'CTMacro_Subj':lambda x: list(x),\n",
    "                                             'CTMeso_Subj':lambda x: list(x),\n",
    "                                             'CTMicro_Subj':lambda x: list(x)\n",
    "                                             }).reset_index()\n",
    "groupby_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export file to excel, without the Pandas index, but with the header\n",
    "\n",
    "groupby_df.to_excel(\"output/groupby.xlsx\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
